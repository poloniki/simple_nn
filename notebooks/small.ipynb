{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Define the input data (15 days of sales, 1 value per day)\n",
    "data = np.array([100, 90, 120, 80, 150, 110, 130, 140, 115, 105, 90, 125, 135, 100, 95])\n",
    "#data = np.arange(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial sales data as a list of numbers\n",
    "data = [10, 12, 15]\n",
    "\n",
    "# Define the mean and standard deviation of the initial sales data\n",
    "mean_sales = np.mean(data)\n",
    "std_sales = np.std(data)\n",
    "\n",
    "# Define a function to generate new sales data based on the last 3 days\n",
    "def generate_new_sales(last_sales):\n",
    "    # Calculate the mean and standard deviation of the last 3 days of sales\n",
    "    last_mean = np.mean(last_sales)\n",
    "    last_std = np.std(last_sales)\n",
    "\n",
    "    # Calculate the new sales value as the mean of the last 3 days plus a random amount\n",
    "    value = np.round(last_mean+(last_mean*0.12))\n",
    "    return value\n",
    "\n",
    "# Generate the next 10 days of sales data based on the initial sales data\n",
    "for i in range(51):\n",
    "    # Get the last 3 days of sales data\n",
    "    last_sales = data[-3:]\n",
    "\n",
    "    # Generate the new sales data based on the last 3 days\n",
    "    new_sales = generate_new_sales(last_sales)\n",
    "\n",
    "    # Append the new sales data to the initial sales list\n",
    "    data.append(new_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data, columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mIce Cream Sales\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(data)\n\u001b[1;32m      3\u001b[0m std \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstd(data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data, columns=['Ice Cream Sales'])\n",
    "mean = np.mean(data)\n",
    "std = np.std(data)\n",
    "\n",
    "# Normalize the array along the columns\n",
    "data = (data - mean) / std\n",
    "\n",
    "df['diff'] = df['Ice Cream Sales'].diff()\n",
    "data = df['diff'].values[1:]\n",
    "df = pd.DataFrame(data, columns=['Diff Sales'])\n",
    "\n",
    "mean = np.mean(data)\n",
    "std = np.std(data)\n",
    "\n",
    "# Normalize the array along the columns\n",
    "data = (data - mean) / std\n",
    "df = pd.DataFrame(data, columns=['Ice Cream Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 2.423294439198322\n",
      "Error: 0.7781158156651444\n",
      "Error: 0.0560992771567041\n",
      "Error: 0.6783169785805674\n",
      "Error: 6.274992805713473\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 2 into shape (3,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m x_batch \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((batch_size, num_time_steps, num_input_features))\n\u001b[1;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size):\n\u001b[0;32m---> 37\u001b[0m     x_batch[j,:,:] \u001b[39m=\u001b[39m data[i\u001b[39m+\u001b[39;49mj:i\u001b[39m+\u001b[39;49mj\u001b[39m+\u001b[39;49mnum_time_steps]\u001b[39m.\u001b[39;49mreshape((num_time_steps, num_input_features))\n\u001b[1;32m     39\u001b[0m \u001b[39m# Compute the new hidden states for the current batch using the current inputs and previous hidden states\u001b[39;00m\n\u001b[1;32m     40\u001b[0m h_t \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((batch_size, num_hidden_units))\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 2 into shape (3,1)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "mean = np.mean(data)\n",
    "std = np.std(data)\n",
    "\n",
    "# Normalize the array along the columns\n",
    "data = (data - mean) / std\n",
    "# Define the number of time steps in the input sequence\n",
    "num_time_steps = 3\n",
    "\n",
    "# Define the number of input features (in this case, we only have 1 input feature: sales)\n",
    "num_input_features = 1\n",
    "\n",
    "# Define the number of hidden units in the RNN layer\n",
    "num_hidden_units = 5\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 10\n",
    "\n",
    "# Initialize the weight matrices and bias vectors for the RNN layer\n",
    "W_xh = np.random.randn(num_input_features, num_hidden_units)\n",
    "W_hh = np.random.randn(num_hidden_units, num_hidden_units)\n",
    "b_h = np.zeros((1, num_hidden_units))\n",
    "W_hy = np.random.randn(num_hidden_units, num_input_features)\n",
    "b_y = np.zeros((1, num_input_features))\n",
    "\n",
    "# Initialize the hidden state (this is the initial state before processing any input)\n",
    "h_t = np.zeros((batch_size, num_hidden_units))\n",
    "\n",
    "# Define the learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "for each in range(1000):\n",
    "# Loop through the input sequence and update the hidden state at each time step\n",
    "    for i in range(0, len(data) - num_time_steps, batch_size):\n",
    "        # Extract the input sequence for the current batch\n",
    "        x_batch = np.zeros((batch_size, num_time_steps, num_input_features))\n",
    "        for j in range(batch_size):\n",
    "            x_batch[j,:,:] = data[i+j:i+j+num_time_steps].reshape((num_time_steps, num_input_features))\n",
    "        \n",
    "        # Compute the new hidden states for the current batch using the current inputs and previous hidden states\n",
    "        h_t = np.zeros((batch_size, num_hidden_units))\n",
    "        for t in range(num_time_steps):\n",
    "            x_t = x_batch[:,t,:]\n",
    "            h_t = np.tanh(np.dot(x_t, W_xh) + np.dot(h_t, W_hh) + b_h)\n",
    "        \n",
    "        # Compute the outputs for the current batch at the last time step\n",
    "        y_pred = np.dot(h_t, W_hy) + b_y\n",
    "        \n",
    "        # Compute the target values for the current batch (in this case, we want to predict the next day's sales based on the previous 3 days of sales)\n",
    "        y_true = data[i+num_time_steps:i+num_time_steps+batch_size].reshape((batch_size, num_input_features))\n",
    "        \n",
    "        # Compute the error (mean squared error) for the current batch\n",
    "        error = 0.5 * np.mean((y_pred - y_true) ** 2)\n",
    "        \n",
    "        # Compute the gradients of the output layer for the current batch (using the chain rule)\n",
    "        grad_y = (y_pred - y_true) / batch_size\n",
    "        grad_W_hy = np.dot(h_t.T, grad_y)\n",
    "        grad_b_y = np.sum(grad_y, axis=0, keepdims=True)\n",
    "        \n",
    "        # Initialize the gradients of the hidden state for the current batch (this will be used as the initial gradients for backpropagation)\n",
    "        grad_h = np.zeros((batch_size, num_hidden_units))\n",
    "        \n",
    "        # Loop backward through the time steps and compute the gradients for each time step for the current batch\n",
    "        for t in reversed(range(num_time_steps)):\n",
    "            x_t = x_batch[:,t,:]\n",
    "            h_t = np.tanh(np.dot(x_t, W_xh) + np.dot(h_t, W_hh) + b_h)\n",
    "            \n",
    "            # Compute the gradients for the output of the RNN layer (using the chain rule)\n",
    "            grad_output = grad_h + np.dot(grad_y, W_hy.T)\n",
    "            grad_z = grad_output * (1 - h_t ** 2)\n",
    "            \n",
    "            # Compute the gradients for the parameters of the RNN layer (using the chain rule)\n",
    "            grad_W_xh = np.dot(x_t.T, grad_z)\n",
    "            grad_W_hh = np.dot(h_t.T, grad_z)\n",
    "            grad_b_h = np.sum(grad_z, axis=0, keepdims=True)\n",
    "            \n",
    "            # Update the gradients for the next time step (using the chain rule)\n",
    "            grad_h = np.dot(grad_z, W_hh.T)\n",
    "            \n",
    "            # Accumulate the gradients for the current batch\n",
    "            if t == num_time_steps - 1:\n",
    "                total_grad_W_xh = grad_W_xh\n",
    "                total_grad_W_hh = grad_W_hh\n",
    "                total_grad_b_h = grad_b_h\n",
    "                total_grad_W_hy = grad_W_hy\n",
    "                total_grad_b_y = grad_b_y\n",
    "            else:\n",
    "                total_grad_W_xh += grad_W_xh\n",
    "                total_grad_W_hh += grad_W_hh\n",
    "                total_grad_b_h += grad_b_h\n",
    "                total_grad_W_hy += grad_W_hy\n",
    "                total_grad_b_y += grad_b_y\n",
    "\n",
    "        # Update the parameters of the RNN layer for the current batch using the computed gradients and the learning rate\n",
    "        W_xh -= learning_rate * total_grad_W_xh\n",
    "        W_hh -= learning_rate * total_grad_W_hh\n",
    "        b_h -= learning_rate * total_grad_b_h\n",
    "        W_hy -= learning_rate * total_grad_W_hy\n",
    "        b_y -= learning_rate * total_grad_b_y\n",
    "        print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for each in range(0, len(data) - num_time_steps, batch_size):\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.45723794],\n",
       "       [ 0.01784452],\n",
       "       [-0.22004888],\n",
       "       [-0.04935979]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 12:40:04.862053: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-15 12:40:05.665679: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-15 12:40:05.665723: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-15 12:40:05.665726: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-15 12:40:06.732099: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-02-15 12:40:06.749024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-02-15 12:40:06.749049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-02-15 12:40:06.749470: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-15 12:40:06.751768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-02-15 12:40:06.751792: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-02-15 12:40:06.751802: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-02-15 12:40:07.408899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-02-15 12:40:07.409154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "upport.\n",
      "2023-02-15 12:40:07.409166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-02-15 12:40:07.409184: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-02-15 12:40:07.409350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9382 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-02-15 12:40:09.009551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-02-15 12:40:09.381720: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x56288f967160 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-02-15 12:40:09.381743: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\n",
      "2023-02-15 12:40:09.393387: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-02-15 12:40:09.516444: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 2s 8ms/step - loss: 1.0829\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0697\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0582\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0470\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0358\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0255\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0162\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0074\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0003\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc40df51540>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, SimpleRNN, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the input shape and parameters of the RNN layer\n",
    "num_time_steps = 3\n",
    "num_input_features = 1\n",
    "num_hidden_units = 5\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(num_time_steps, num_input_features))\n",
    "\n",
    "# Define the RNN layer\n",
    "rnn_layer = SimpleRNN(units=num_hidden_units, activation='tanh')\n",
    "\n",
    "# Define the output layer\n",
    "output_layer = Dense(units=1)\n",
    "\n",
    "# Connect the input layer to the RNN layer\n",
    "rnn_output = rnn_layer(input_layer)\n",
    "\n",
    "# Connect the RNN layer to the output layer\n",
    "output = output_layer(rnn_output)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Generate some random data\n",
    "x = np.random.randn(100, num_time_steps, num_input_features)\n",
    "y = np.random.randn(100, 1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(x, y, epochs=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoweb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9a78c674d694199934c1a8e7fc588a9a802eb89d75a3f9a207bd39d883dc205"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
