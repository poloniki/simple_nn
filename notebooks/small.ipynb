{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the input data (15 days of sales, 1 value per day)\n",
    "data = np.array([100, 90, 120, 80, 150, 110, 130, 140, 115, 105, 90, 125, 135, 100, 95])\n",
    "\n",
    "# Define the number of time steps in the input sequence\n",
    "num_time_steps = 3\n",
    "\n",
    "# Define the number of input features (in this case, we only have 1 input feature: sales)\n",
    "num_input_features = 1\n",
    "\n",
    "# Define the number of hidden units in the RNN layer\n",
    "num_hidden_units = 5\n",
    "\n",
    "# Initialize the weight matrices and bias vectors for the RNN layer\n",
    "W_xh = np.random.randn(num_input_features, num_hidden_units)\n",
    "W_hh = np.random.randn(num_hidden_units, num_hidden_units)\n",
    "b_h = np.zeros((1, num_hidden_units))\n",
    "W_hy = np.random.randn(num_hidden_units, num_input_features)\n",
    "b_y = np.zeros((1, num_input_features))\n",
    "\n",
    "# Initialize the hidden state (this is the initial state before processing any input)\n",
    "h_t = np.zeros((1, num_hidden_units))\n",
    "\n",
    "# Define the learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Loop through the input sequence and update the hidden state at each time step\n",
    "for t in range(num_time_steps):\n",
    "    # Extract the input features for the current time step\n",
    "    x_t = data[t]\n",
    "    \n",
    "    # Compute the new hidden state using the current input and previous hidden state\n",
    "    h_t = np.tanh(np.dot(x_t, W_xh) + np.dot(h_t, W_hh) + b_h)\n",
    "    \n",
    "# Compute the output at the last time step\n",
    "y_pred = np.dot(h_t, W_hy) + b_y\n",
    "\n",
    "# Compute the target values (in this case, we want to predict the next day's sales based on the previous 3 days of sales)\n",
    "y_true = data[num_time_steps:num_time_steps+1]\n",
    "\n",
    "# Compute the error (mean squared error)\n",
    "error = 0.5 * np.sum((y_pred - y_true) ** 2)\n",
    "\n",
    "# Compute the gradients of the output layer (using the chain rule)\n",
    "grad_y = y_pred - y_true\n",
    "grad_W_hy = np.dot(h_t.T, grad_y)\n",
    "grad_b_y = np.sum(grad_y, axis=0, keepdims=True)\n",
    "\n",
    "# Initialize the gradient of the hidden state (this will be used as the initial gradient for backpropagation)\n",
    "grad_h = np.zeros((1, num_hidden_units))\n",
    "\n",
    "# Loop backward through the time steps and compute the gradients for each time step\n",
    "for t in reversed(range(num_time_steps)):\n",
    "    # Extract the input features for the current time step\n",
    "    x_t = data[t]\n",
    "    \n",
    "    # Compute the gradients for the output of the RNN layer (using the chain rule)\n",
    "    grad_output = grad_h + np.dot(grad_y, W_hy.T)\n",
    "    grad_z = grad_output * (1 - h_t ** 2)\n",
    "    \n",
    "    # Compute the gradients for the parameters of the RNN layer (using the chain rule)\n",
    "    grad_W_xh = np.dot(x_t.T, grad_z)\n",
    "    grad_W_hh = np.dot(h_t.T, grad_z)\n",
    "    grad_b_h = np.sum(grad_z, axis=0, keepdims=True)\n",
    "    \n",
    "    # Update the gradients for the next time step (using the chain rule)\n",
    "    grad_h = np.dot(grad_z,W_hh.T)\n",
    "\n",
    "# Update the parameters of the RNN layer using the computed gradients and the learning rate\n",
    "W_xh -= learning_rate * grad_W_xh\n",
    "W_hh -= learning_rate * grad_W_hh\n",
    "b_h -= learning_rate * grad_b_h\n",
    "W_hy -= learning_rate * grad_W_hy\n",
    "b_y -= learning_rate * grad_b_y\n",
    "\n",
    "print(\"W_xh:\", W_xh)\n",
    "print(\"W_hh:\", W_hh)\n",
    "print(\"b_h:\", b_h)\n",
    "print(\"W_hy:\", W_hy)\n",
    "print(\"b_y:\", b_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 6845.298336578325\n",
      "Error: 5806.473069099888\n",
      "Error: 5643.544407327385\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the input data (15 days of sales, 1 value per day)\n",
    "data = np.array([100, 90, 120, 80, 150, 110, 130, 140, 115, 105, 90, 125, 135, 100, 95])\n",
    "\n",
    "# Define the number of time steps in the input sequence\n",
    "num_time_steps = 3\n",
    "\n",
    "# Define the number of input features (in this case, we only have 1 input feature: sales)\n",
    "num_input_features = 1\n",
    "\n",
    "# Define the number of hidden units in the RNN layer\n",
    "num_hidden_units = 5\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 4\n",
    "\n",
    "# Initialize the weight matrices and bias vectors for the RNN layer\n",
    "W_xh = np.random.randn(num_input_features, num_hidden_units)\n",
    "W_hh = np.random.randn(num_hidden_units, num_hidden_units)\n",
    "b_h = np.zeros((1, num_hidden_units))\n",
    "W_hy = np.random.randn(num_hidden_units, num_input_features)\n",
    "b_y = np.zeros((1, num_input_features))\n",
    "\n",
    "# Initialize the hidden state (this is the initial state before processing any input)\n",
    "h_t = np.zeros((batch_size, num_hidden_units))\n",
    "\n",
    "# Define the learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Loop through the input sequence and update the hidden state at each time step\n",
    "for i in range(0, len(data) - num_time_steps, batch_size):\n",
    "    # Extract the input sequence for the current batch\n",
    "    x_batch = np.zeros((batch_size, num_time_steps, num_input_features))\n",
    "    for j in range(batch_size):\n",
    "        x_batch[j,:,:] = data[i+j:i+j+num_time_steps].reshape((num_time_steps, num_input_features))\n",
    "    \n",
    "    # Compute the new hidden states for the current batch using the current inputs and previous hidden states\n",
    "    h_t = np.zeros((batch_size, num_hidden_units))\n",
    "    for t in range(num_time_steps):\n",
    "        x_t = x_batch[:,t,:]\n",
    "        h_t = np.tanh(np.dot(x_t, W_xh) + np.dot(h_t, W_hh) + b_h)\n",
    "    \n",
    "    # Compute the outputs for the current batch at the last time step\n",
    "    y_pred = np.dot(h_t, W_hy) + b_y\n",
    "    \n",
    "    # Compute the target values for the current batch (in this case, we want to predict the next day's sales based on the previous 3 days of sales)\n",
    "    y_true = data[i+num_time_steps:i+num_time_steps+batch_size].reshape((batch_size, num_input_features))\n",
    "    \n",
    "    # Compute the error (mean squared error) for the current batch\n",
    "    error = 0.5 * np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    # Compute the gradients of the output layer for the current batch (using the chain rule)\n",
    "    grad_y = (y_pred - y_true) / batch_size\n",
    "    grad_W_hy = np.dot(h_t.T, grad_y)\n",
    "    grad_b_y = np.sum(grad_y, axis=0, keepdims=True)\n",
    "    \n",
    "    # Initialize the gradients of the hidden state for the current batch (this will be used as the initial gradients for backpropagation)\n",
    "    grad_h = np.zeros((batch_size, num_hidden_units))\n",
    "    \n",
    "    # Loop backward through the time steps and compute the gradients for each time step for the current batch\n",
    "    for t in reversed(range(num_time_steps)):\n",
    "        x_t = x_batch[:,t,:]\n",
    "        h_t = np.tanh(np.dot(x_t, W_xh) + np.dot(h_t, W_hh) + b_h)\n",
    "        \n",
    "        # Compute the gradients for the output of the RNN layer (using the chain rule)\n",
    "        grad_output = grad_h + np.dot(grad_y, W_hy.T)\n",
    "        grad_z = grad_output * (1 - h_t ** 2)\n",
    "        \n",
    "        # Compute the gradients for the parameters of the RNN layer (using the chain rule)\n",
    "        grad_W_xh = np.dot(x_t.T, grad_z)\n",
    "        grad_W_hh = np.dot(h_t.T, grad_z)\n",
    "        grad_b_h = np.sum(grad_z, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update the gradients for the next time step (using the chain rule)\n",
    "        grad_h = np.dot(grad_z, W_hh.T)\n",
    "        \n",
    "        # Accumulate the gradients for the current batch\n",
    "        if t == num_time_steps - 1:\n",
    "            total_grad_W_xh = grad_W_xh\n",
    "            total_grad_W_hh = grad_W_hh\n",
    "            total_grad_b_h = grad_b_h\n",
    "            total_grad_W_hy = grad_W_hy\n",
    "            total_grad_b_y = grad_b_y\n",
    "        else:\n",
    "            total_grad_W_xh += grad_W_xh\n",
    "            total_grad_W_hh += grad_W_hh\n",
    "            total_grad_b_h += grad_b_h\n",
    "            total_grad_W_hy += grad_W_hy\n",
    "            total_grad_b_y += grad_b_y\n",
    "\n",
    "    # Update the parameters of the RNN layer for the current batch using the computed gradients and the learning rate\n",
    "    W_xh -= learning_rate * total_grad_W_xh\n",
    "    W_hh -= learning_rate * total_grad_W_hh\n",
    "    b_h -= learning_rate * total_grad_b_h\n",
    "    W_hy -= learning_rate * total_grad_W_hy\n",
    "    b_y -= learning_rate * total_grad_b_y\n",
    "    print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoweb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9a78c674d694199934c1a8e7fc588a9a802eb89d75a3f9a207bd39d883dc205"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
